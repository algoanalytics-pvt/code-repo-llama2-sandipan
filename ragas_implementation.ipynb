{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ragha\\AppData\\Roaming\\Python\\Python310\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "import os\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import faithfulness,context_precision,answer_relevancy,context_recall,context_utilization, answer_correctness\n",
    "from langchain_community.chat_models import ChatAnyscale\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from utils.prompt_template_utils import get_prompt_template\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "ANYSCALE_API_KEY = \"\"\n",
    "os.environ[\"ANYSCALE_API_BASE\"] = \"https://api.endpoints.anyscale.com/v1\"\n",
    "os.environ[\"ANYSCALE_API_KEY\"] = ANYSCALE_API_KEY \n",
    "OPENAI_API_KEY = \"\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create LLama2 chatbot for evaluation, GPT model for ground_truth generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for ChatAnyscale\n__root__\n  Model name codellama/CodeLlama-13b-Instruct-hf not found in available models: {'mlabonne/NeuralHermes-2.5-Mistral-7B', 'codellama/CodeLlama-70b-Instruct-hf', 'mistralai/Mixtral-8x7B-Instruct-v0.1', 'google/gemma-7b-it', 'meta-llama/Llama-2-7b-chat-hf', 'meta-llama/Llama-2-13b-chat-hf', 'meta-llama/Llama-2-70b-chat-hf', 'meta-llama/Llama-2-7b-chat-hf:algoanalytics:nS5JS3G', 'BAAI/bge-large-en-v1.5', 'thenlper/gte-large', 'mistralai/Mistral-7B-Instruct-v0.1'}. (type=value_error)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# ANYSCALE_MODEL_NAME = \"meta-llama/Llama-2-7b-chat-hf\"\u001b[39;00m\n\u001b[0;32m     10\u001b[0m ANYSCALE_MODEL_NAME \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcodellama/CodeLlama-13b-Instruct-hf\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 11\u001b[0m LLM \u001b[38;5;241m=\u001b[39m \u001b[43mChatAnyscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mANYSCALE_MODEL_NAME\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m LLM2 \u001b[38;5;241m=\u001b[39m ChatOpenAI(model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-3.5-turbo\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     13\u001b[0m retriever \u001b[38;5;241m=\u001b[39m vectorstore\u001b[38;5;241m.\u001b[39mas_retriever(search_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimilarity\u001b[39m\u001b[38;5;124m\"\u001b[39m, search_kwargs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mk\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;241m3\u001b[39m})\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\langchain_core\\_api\\deprecation.py:180\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    178\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    179\u001b[0m     emit_warning()\n\u001b[1;32m--> 180\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\langchain_core\\load\\serializable.py:120\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 120\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lc_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pydantic\\main.py:341\u001b[0m, in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValidationError\u001b[0m: 1 validation error for ChatAnyscale\n__root__\n  Model name codellama/CodeLlama-13b-Instruct-hf not found in available models: {'mlabonne/NeuralHermes-2.5-Mistral-7B', 'codellama/CodeLlama-70b-Instruct-hf', 'mistralai/Mixtral-8x7B-Instruct-v0.1', 'google/gemma-7b-it', 'meta-llama/Llama-2-7b-chat-hf', 'meta-llama/Llama-2-13b-chat-hf', 'meta-llama/Llama-2-70b-chat-hf', 'meta-llama/Llama-2-7b-chat-hf:algoanalytics:nS5JS3G', 'BAAI/bge-large-en-v1.5', 'thenlper/gte-large', 'mistralai/Mistral-7B-Instruct-v0.1'}. (type=value_error)"
     ]
    }
   ],
   "source": [
    "final_embeddings = np.load(f\"instructor_embeddings_large.npy\", allow_pickle=True)\n",
    "vectorstore = final_embeddings.item()\n",
    "\n",
    "template = \"\"\"You are a software engineer answering to a senior software engineer who is testing your understaing of the code provided, you will use the provided knowledge to answer questions about the code. Think step by step and respond appropriately. If you can not answer a question based on \n",
    "the provided context, inform the user. Do not use any other prior information for answering. Give fully explained answers using the terms used in the code itself. A bulleted format is preferred but not necessary. Do not narrate the conversation.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# ANYSCALE_MODEL_NAME = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "ANYSCALE_MODEL_NAME = \"codellama/CodeLlama-13b-Instruct-hf\"\n",
    "LLM = ChatAnyscale(model_name = ANYSCALE_MODEL_NAME)\n",
    "LLM2 = ChatOpenAI(model_name=\"gpt-3.5-turbo\")\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":3})\n",
    "\n",
    "prompt, memory = get_prompt_template(system_prompt=template, promptTemplate_type=\"llama\", history=False)\n",
    "\n",
    "qa_llama = RetrievalQA.from_chain_type(\n",
    "    llm=LLM,\n",
    "    chain_type=\"stuff\",  # try other chains types as well. refine, map_reduce, map_rerank\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True, # verbose=True,\n",
    "    chain_type_kwargs={\"prompt\": prompt, \"memory\": memory},\n",
    "    )\n",
    "\n",
    "qa_gpt = RetrievalQA.from_chain_type(\n",
    "    llm=LLM2,\n",
    "    chain_type=\"stuff\",  # try other chains types as well. refine, map_reduce, map_rerank\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True, # verbose=True,\n",
    "    chain_type_kwargs={\"prompt\": prompt, \"memory\": memory},\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the dataset for RAGAS implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = []\n",
    "contexts = []\n",
    "file = open(\"model\\sample_questions.txt\", 'r')\n",
    "for line in file:\n",
    "    questions.append(line.strip())\n",
    "file.close()\n",
    "\n",
    "for ques in questions:\n",
    "    contexts.append([docs.page_content for docs in retriever.get_relevant_documents(ques)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To regenerate ground truths using GPT 3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truths = [\"- Bubble sort is a simple sorting algorithm that repeatedly steps through the list to be sorted, compares each pair of adjacent items and swaps them if they are in the wrong order.\\n- In this case, the function bubble_sort() is being called with the argument 'elements' which is presumably a list of elements to be sorted.\\n- The function also takes an optional argument 'key' which specifies the key to use for sorting the elements. In this case, the key is 'transaction_amount'.\\n- The sorted list of elements will be printed out after the sorting process is completed.\", '- The @time_it wrapper is a decorator function that calculates the time taken for a function to execute.\\n- It takes a function as input and returns a wrapper function.\\n- Inside the wrapper function, it records the start time before calling the original function, then records the end time after the function has executed.\\n- It calculates the time taken for the function to execute by subtracting the start time from the end time and multiplying by 1000 to get the result in milliseconds.\\n- Finally, it prints out the name of the function and the time taken in milliseconds.\\n- The wrapper function then returns the result of the original function.', '- The `partition()` function takes in three parameters: `elements`, `start`, and `end`.\\n- It initializes the `pivot` variable to be the last element in the `elements` list.\\n- It also initializes the `p_index` variable to the value of `start`.\\n- It then iterates through the elements of the list from index `start` to `end - 1`.\\n- For each element, if the element is less than or equal to the `pivot`, it calls the `swap()` function passing the current index `i`, the `p_index`, and the `elements` list as arguments. This is done to move the smaller elements to the left side of the pivot.\\n- After the loop, it swaps the `p_index` element with the `end` element to place the pivot in its correct sorted position.\\n- Finally, it returns the `p_index`, which represents the index where the pivot element is placed in the sorted list.', '- BFS stands for Breadth First Search, while DFS stands for Depth First Search.\\n- In BFS, the nodes are visited level by level starting from the root node, while in DFS, the nodes are visited depth-wise until reaching the leaf nodes.\\n- BFS uses a queue data structure to keep track of the nodes to be visited, while DFS uses a stack or recursion to keep track of nodes.\\n- BFS is better suited for finding the shortest path between two nodes in an unweighted graph, while DFS is more appropriate for topological sorting, cycle detection, and traversal of connected components.', '- The `fib()` function calculates and returns the nth Fibonacci number.\\n- The Fibonacci sequence starts with 0 and 1, and each subsequent number is the sum of the two preceding numbers.\\n- So, for example, if you call `fib(10)`, it will return the 10th Fibonacci number in the sequence.', '- The code defines a list of lists called \"tests\" containing various integer elements lists.\\n- It then iterates over each list in the \"tests\" list using a for loop.\\n- Within the loop, it calls the function \"quick_sort\" with the current list, the starting index 0, and the ending index which is the length of the list minus 1.\\n- The \"quick_sort\" function sorts the elements in the list using the Quick Sort algorithm.\\n- After sorting, it prints out the sorted array for each list in the \"tests\" list.\\n\\nOverall, the code sorts each list of integers in the \"tests\" list using the Quick Sort algorithm and prints out the sorted arrays.', '- The variable \"pivot\" in the function partition() stores the last element in the list \"elements\" which is used as the pivot for the partitioning process.\\n- It is compared to other elements in the list to determine whether they should be placed before or after the pivot element.\\n- The elements smaller than or equal to the pivot are placed before it, and the elements greater than the pivot are placed after it.', '- The `binary_search()` function is an iterative implementation of the binary search algorithm, while the `binary_search_recursive()` function is a recursive implementation of the binary search algorithm.\\n- In the iterative `binary_search()` function, the algorithm uses a while loop to repeatedly divide the list into smaller parts and compare the middle element with the target value until the target value is found or the list is exhausted.\\n- In the recursive `binary_search_recursive()` function, the algorithm calls itself with updated parameters for the left and right indices until the target value is found or the base case is reached.\\n- Both functions follow the same binary search algorithm logic, but the difference lies in their implementation approach â€“ iterative vs. recursive.', '- The code defines a function called merge_sort that takes an array as input.\\n- Inside the merge_sort function, it checks if the length of the array is less than or equal to 1, in which case it returns the array as is.\\n- If the length of the array is greater than 1, it calculates the middle index of the array and splits the array into two halves.\\n- It then recursively calls the merge_sort function on the two halves of the array.\\n- After the recursive calls, it returns the result of merging the two sorted halves using the merge_two_sorted_lists function.\\n\\nExample:\\n- Given array: [10, 3, 15, 7, 8, 23, 98, 29]\\n- First, the array is split into [10, 3, 15, 7] and [8, 23, 98, 29].\\n- Then, the two halves are further split into [10, 3] and [15, 7] and [8, 23] and [98, 29].\\n- This process continues until each subarray has only one element.\\n- The merging process starts by merging [10] and [3] to get [3, 10].\\n- Then [15] and [7] are merged to get [7, 15].\\n- The final merge step combines [3, 10] and [7, 15] to get [3, 7, 10, 15].\\n- This process is repeated for the second half of the array.\\n- Finally, the two sorted halves [3, 7, 10, 15] and [8, 23, 29, 98] are merged to get the fully sorted array [3, 7, 8, 10, 15, 23, 29, 98].\\n- The sorted array is then printed.', \"- The shell_sort() function can be optimized by changing the gap sequence used for sorting.\\n- One way to optimize it is to use the Knuth's sequence for determining the gap values.\\n- Knuth's sequence is defined as h = (3 * h) + 1 where h is the starting value.\\n- By using Knuth's sequence, the shell_sort() function can achieve better performance by reducing the number of comparisons and swaps required.\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To regenerate answers from LLAMA2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = []\n",
    "for ques in questions:\n",
    "    answers.append(qa_llama.invoke(ques)['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compiling dataset into dictionary for RAGAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\"question\": questions,\n",
    "        \"answer\": answers,\n",
    "        \"contexts\": contexts,\n",
    "        \"ground_truth\": ground_truths}\n",
    "\n",
    "dataset = Dataset.from_dict(data)\n",
    "\n",
    "ragas_results = evaluate(\n",
    "    dataset = dataset,\n",
    "    metrics=[\n",
    "        context_utilization,\n",
    "        faithfulness,\n",
    "        answer_relevancy,\n",
    "        context_precision,\n",
    "        context_recall,\n",
    "        answer_correctness\n",
    "    ]\n",
    ")\n",
    "\n",
    "df = ragas_results.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df)\n",
    "df.to_csv(\"ragas_evauation_codellama_bgelarge.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
